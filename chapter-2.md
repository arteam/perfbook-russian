# Глава 2
## Введение

Параллельное программирование заслужило репутацию как одна из самых сложных областей, с которыми программист может столкнуться. Научные работы и книги предупреждают об опасностях деддлока, лайвлока, гонки условий, неопределенности, ограничений закона Амдала по масштабируемости и чрезмерных задержках в реальном времени. И эти опасности вполне реальны; мы, авторы, собрали неисчислимые года опыта работы с ними: все эмоциональные шрамы, седые волосы и их потерю, которая появляется после встречи с такими опытами.

Однако, новые технологии, которые сложны для использовании в при появлении, неизбежно становятся проще со временем. Например, в свое время редкое умение водить машину сейчас является обычной вещью во многих странах. Такое большое изменение произошло по двум базовым причинам: (1) машины стали дешевле и более легко доступны, таким образом большее количество людей получило возможность научится водить и (2) машины стали более просты в эксплуатации благодаря автоматическим трансмиссиям, автоматическим дросселям, автоматическим стартерам, значительно увеличинной надежности и набору других технологических улучшений.

Это же является правдой и для других технологий, включая компьютеры. Теперь не нужно работать с перфокартами для того, чтобы программировать.
Умные таблицы позволяют большинству непрограммистов получать такие результаты от их компьютеров, получение которых требовало бы команду специалистов несколько десятков лет назад. Возможно, наиболее яркий пример это веб-серфинг и создание контента. С начала 2000-x с помощью разных распространенных социально-сетевых инструментов это могут делать люди без специального обучения и тренировки. Не так давно как в 1968 году, такое создание контента было далеким исследовательским проектом [Eng68], описываемым в свое время как "приземление летающей тарелки на лужайку перед Белым домом".

Следовательно, если вы желаете спорить, что параллельное программирование останется таким же сложным как оно воспринимается многими сейчас, это вы, кто должен приводить доказательства, при этом не забывая о многих веках контр-примеров в различных областях человеческих достижений. 

### Исторические сложности параллельного программирования

Как следует из ее названия, это книга преследует другой подход. Вместо того, чтобы жаловаться на сложности параллельного программирования, она вместе этого изучает причины, почему же параллельного программирование сложно и помогает читателю преодолеть эти сложности. Как мы дальше увидим, эти сложности разделяются на несколько категорий, включая:

1. Исторически высокую цену и относительную редкость параллельных систем;
2. Отсутствие опыта работы с параллельными системами у типичных исследователей и практиков;
3. Малое количество публично доступного параллельного кода;
4. Отсуствие широко понимаемой инженерной дисциплины параллельного программирования;
5. Высокие накладные расходы на коммуникацию по сравнению с обработкой данных, даже в сильно связанных вычислительных системах с разделяемой памятью.

Многие из этих исторических сложностей уже на пути к преодолению. 
1. За последние несколько десятков лет благодаря закону Мура цена параллельных систем упала с порядка цены на дом до порядка цены на велосипед. Научные работы, говорящие о преимуществах мультиядерных процессоров были опубликованы в начале 1996 года [OHN+96]. IBM одновременно внедрила многопоточность в передовую линейку процессоров POWER в 2000 и мультиядерность в 2001. Intel внедрила гиперпоточность в их двухядерные процессоры в 2005. Sun последовала за ними с мультиядерным/мультипоточным процессором Niagara в конце 2005. На самом деле, в 2008 году уже было сложно найти однопроцесоррную настольную систему, а одноядерные процессоры остались только в нетбуках и встроенных устройствах. К 2012 году даже смартфоны начали выпускаться с несколькими процессорами.

2. Приход дешевых и широкодоступных мультиядерных систем, значит, что в свое время редкий опыт параллельного программирования теперь доступен практически каждому исследователю и практику. На самом деле, параллельные системы сейчас по карману даже школьнику и любителю. Мы, следовательно, можем ожидать большое увеличение уровня открытий и инноваций в областях, окружающих параллельные системы. Эта увеличивающаяся знакомость через некоторе время сделает однажды ужасно дорогую область параллельного программирования более дружелюбной и обычной. 

3. В 20 веке, большинство большие системы с используем высоко распаралезированного программного обеспечения были закрытыми и тщательно охраняемыми проприетарными секретами. На контрасте с этим, 21 век увидел многочисленные проекты параллельного программного обеспечния с открытым кодом (следовательно  публично доступых), включая ядро Linux [Tor03c], системы управления базами данных [Pos08, MS08] и систем передач сообщений [The08, UoC08]. Эта книга в основном будет сфокусирована на ядре Linux, по предоставит много материала, подходящего и для прикладных приложений.

4. Даже несмотря н то, что большинство проектов параллельного программирования в 1980-х и в 1990-х были проприетарными, эти проекты дали жизнь сообществу разработчиков, кто понимает инженерную дисциплину, необходимую для разработки параллельного кода производственного качества. Одна из главных целей этой книги показать эту инжереную дисциплину.

5. К сожалению, с пятой проблемой, высокой ценой коммуникации по отнощению к обработке данных, все еще борятся. Хотя к этой проблеме было приковано много внимания с начала нового тысячилетия, согласно Стивену Хокингу, конечная скорость света и атомная природа материи, скорее всего ограничит прогресс в этой области [Gar 07, Moo03]. К счастью, с этой проблемой борятся с конца 1980-х годов, так что вышеупомянутая инженерная дисциплина разработала практичные и эффективные стратегии для ее обхождения. В добавок, проектировщики аппаратных обеспечения все более знакомы с этими проблемами, так что возможно будущее аппаратное обеспечение будет более дружелюбно к параллельному программному обеспечению, как было обсуждается в разделе 3.3.

**Быстрый вопрос 2.1:** Ну хватит!! Параллельное программирование было невооброзимо сложным многие десятилетия. Вы же, судя по всему, указывате на то, что оно не так сложно. На что вы намекаете?
Несмотря на то, что параллельное программирование может и не так сложно, как оно преподносится, оно все еще сложнее, чем последовательное программирование.

**Быстрый вопрос 2.2:** Как параллельное программирование вообще может быть таким же легким, как последовательное программирование?
Следовательно, имеет смысл рассмотреть альтернативы параллельноому программированию. Однако, невозможно аргументировано рассматривать альтернативы параллельному программированию без понимания его целей. Эта тема рассматривается в следующем разделе.

## Цели параллельного программирования

Три главные цели параллельного программирования(в дополнение к целям последовательного программирования):
1. Производительность;
2. Эффективность;
3. Общность

**Быстрый вопрос 2.3:** Серьезно??? А что насчет корректности, поддерживаемости, надежности и так далее?

**Быстрый вопрос 2.4:** И если корректность, поддерживаемость и надежность не попали в список, почему попали эффективность и общность?

**Быстрый вопрос 2.5:** Учитывая, что  намного сложнее доказать корректность параллельных программ, чем последовательных, опять, не должна ли все-таки корректность быть в списке?

**Быстрый вопрос 2.6:** Как насчет того, чтобы просто весело проводить время?
Каждая из этих целей тщательно разобрана в следующих разделах:

### 2.2.1 Производительность
Производительность - это главная цель большинства усилий, связанных с параллельным программированием. В конце концов, если производительность не забота, почему бы не сделать себе услугу: просто писать последовательный код и быть счастливым? Это будет намного проще и скорее всего вы сделате намного больше вещей быстрее.

**Быстрый вопрос 2.7:** Разве не существует ситуаций, когда парралельное программирование, это не о производительности?
Имейте ввиду, что "производительность" толкуется здесь довольно широко, включая как масшатабируемость(производильность каждого процессору), так и эффективность(например, производительность на ватт энергии).

Тем не менее, фокус производительности сдвинулся с аппаратного обеспечения к параллельному программному обеспечению. Этот сдвиг происходит из-за того, что несмотря на то, что закон Мура продолжает доставлять увелечение плотности транзисторов, он перестал доставлять увелечение производителоности для традиционных однопоточных программ. Это можно увидеть на графике 2.1, который показывает, что написание однопоточного кода и простое ожидание того, что через год или два процессор догонится до нужной производительности может быть уже не вариантом. Учитывая последние тенденции всех главных производителей по отношению к мультиядерным/мультипоточным системам, параллеизм это выход для тех, кто хочет достичь максимальной производительность от их систем.

![График 2.1](../master/clockfreq.png?raw=true)

График 2.1: Тенденция показателя MIPS/тактовая частота для процессоров Intel

Даже так, производительность является более важной, чем масштабируемость, особенно учитывая, что наиболее простой способ достичь линейной масштабируемости - это понизить производительность каждого процессора [Tor01]. Если вам дана 4-процессорная система, что вы предпочтете: программу, которая выполняет 100 транзакций в секунду на одном процессору, но не масштабируется или программу, которая выполняет 10 транзкаций в секунду, но отлично масштабируется. Первый вариант смотрится лучше, однако ответ может изменится, если у вас появится 32-процессорная система. 

Однако, только потому что у вас есть несколько процессоров, вы не обязаны все их использовать, особенно учитывая недавние подешевение цен на мультипроцессорные системы. Ключевая идея в том, что параллельное программирование, в основном это оптимизация производительности и, следовательно, все лишь возможная оптимизация из многих доступных. Если ваша программа достаточна быстра в таком виде, в котором она сейчас написана, нет причины оптимизировать ее, как путем распараллеливания, так и путем применения потенциальных оптимизация для последовательных программ. Кстати, если вы собираетесь применить паралеллизм как оптимизацию к последовательной программе, вам надо будет сравнивать параллельные алогритмы с лучшими последовательными. Это может потребовать некоторого внимания, так как слишком много работ игнорируют последовательные аналоги при анализе производительности параллельных алгоритмов.

